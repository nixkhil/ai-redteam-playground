# ğŸ§ª AI Red Team Labs

AI Red Team Labs is a **living, adaptive red-teaming laboratory** for Large Language Models (LLMs).

It is designed to:
- simulate real-world adversarial pressure
- evolve attacks based on past success
- explain *why* a model failed, not just *that* it failed
- serve as a teachable unit for learning LLM security

This is not a static playground.  
The system **learns as you use it**.

---

## ğŸš€ Features

### ğŸ§¬ Evolutionary Attack Mutation Engine
- Mutations compete based on success
- Effective attack strategies evolve
- Weak strategies decay over time

### ğŸ§  Explainable Red-Team Analysis
Each run reports:
- **Attack Surface Compromised**
- **Exploited Mechanism**
- Clear defensive failure points

### ğŸ›ï¸ Mutation Control Modes
- **Adaptive (Evolutionary)** â€“ lab controls escalation
- **Guided** â€“ select mutation families
- **Freeform** â€“ manual payload testing

### ğŸ“š Attack Memory
- Every run is logged
- Patterns emerge over time
- Enables longitudinal learning

### ğŸ“± Mobile-Friendly UI
- Optimized for iOS Safari
- Touch-friendly layout
- Responsive design

---

## ğŸ—ï¸ Project Structure
ai-red-team-labs/
â”œâ”€â”€ app.py
â”œâ”€â”€ attacks/
â”‚ â””â”€â”€ jailbreaks.json
â”œâ”€â”€ assets/
â”‚ â””â”€â”€ background.jpg # 1920x1080 recommended
â”œâ”€â”€ attack_memory.json
â”œâ”€â”€ mutation_pool.json
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md


---

## âš™ï¸ Installation

### 1. Create virtual environment
```bash
python -m venv .venv
.venv\Scripts\activate   # Windows
source .venv/bin/activate # macOS/Linux
ollama pull mistral:instruct
python -m streamlit run app.py